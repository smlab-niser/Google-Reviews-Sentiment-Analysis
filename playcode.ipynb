{"cells":[{"metadata":{"id":"BZF6eSJHfsl-","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import GridSearchCV\n#NB\nfrom sklearn.metrics import f1_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\n\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom wordcloud import WordCloud","execution_count":null,"outputs":[]},{"metadata":{"id":"YIWmEYeIfsmC","colab_type":"code","outputId":"4e505258-9b76-4fc1-d6ae-3cfd3a9f33b9","colab":{},"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"gps.csv\")\ndf1 = pd.read_csv(\"gpst.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"u4as4mJofsmH","colab_type":"text"},"cell_type":"markdown","source":" ## Data Preprocessing"},{"metadata":{"id":"GNsd3P-mfsmI","colab_type":"code","outputId":"5966e162-4faf-4cca-c6e0-60f78f02e7b9","colab":{},"trusted":true},"cell_type":"code","source":"\nprint(df.shape)\n\nprint(df.isnull().values.any())","execution_count":null,"outputs":[]},{"metadata":{"id":"40MROLk3fsmN","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"df=df.drop(['Sentiment_Polarity', 'Sentiment_Subjectivity'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"TN_9WLwmfsmQ","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":" \nfinal = df.dropna(how='any',axis=0) ","execution_count":null,"outputs":[]},{"metadata":{"id":"65T3hSwQfsmS","colab_type":"code","outputId":"9aadf884-a198-450a-afbb-28ee182f21da","colab":{},"trusted":true},"cell_type":"code","source":"\nprint(final.shape)\n\nprint(final.isnull().values.any())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n## graph\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,5))\nfig = sns.countplot(x=df['Department'], palette=\"hls\")\nfig = sns.countplot(x=df['Category'], palette=\"hls\")\nfig = sns.countplot(x=df['Ratings'], palette=\"hls\")\nfig = sns.countplot(x=df['Type'], palette=\"hls\")\n\nfig.set_xticklabels(fig.get_xticklabels(),rotation=90)\nplt.show(fig)\n\n##graph\nplt.figure(figsize = (10,10))\nsns.regplot(x=\"Price\", y=\"Rating\", color = 'blue',data=data[data['Reviews']<1000000]);\n\nplt.figure(figsize = (10,10))\nsns.regplot(x=\"Size\", y=\"Installs\", color = 'blue',data=data[data['Reviews']<1000000]);\n\n\n##graph\nnew_df = df.groupby(['Category', 'Type']).agg({'App' : 'count'}).reset_index()\n#print(new_df)\n\n# outer_group_names = df['Category'].sort_values().value_counts()[:5].index\n# outer_group_values = df['Category'].sort_values().value_counts()[:5].values\n\nouter_group_names = ['SOCIAL', 'FINANCE', 'SPORTS', 'SHOPPING']\nouter_group_values = [len(df.App[df.Category == category]) for category in outer_group_names]\n\na, b, c, d=[plt.cm.Blues, plt.cm.Reds, plt.cm.Greens, plt.cm.Purples]\n\n\ninner_group_names = ['Paid', 'Free'] * 4\ninner_group_values = []\n#inner_colors = ['#58a27c','#FFD433']\n\n\nfor category in outer_group_names:\n    for t in ['Paid', 'Free']:\n        x = new_df[new_df.Category == category]\n        try:\n            #print(x.App[x.Type == t].values[0])\n            inner_group_values.append(int(x.App[x.Type == t].values[0]))\n        except:\n            #print(x.App[x.Type == t].values[0])\n            inner_group_values.append(0)\n\nexplode = (0.025,0.025,0.025,0.025)\n# First Ring (outside)\nfig, ax = plt.subplots(figsize=(10,10))\nax.axis('equal')\nmypie, texts, _ = ax.pie(outer_group_values, radius=1.2, labels=outer_group_names, autopct='%1.1f%%', pctdistance=1.1,\n                                 labeldistance= 0.75,  explode = explode, colors=[a(0.6), b(0.6), c(0.6), d(0.6)], textprops={'fontsize': 16})\nplt.setp( mypie, width=0.5, edgecolor='black')\n \n\nmypie2, _ = ax.pie(inner_group_values, radius=1.2-0.5, labels=inner_group_names, labeldistance= 0.7, \n                   textprops={'fontsize': 12}, colors = [a(0.4), a(0.2), b(0.4), b(0.2), c(0.4), c(0.2), d(0.4), d(0.2)])\nplt.setp( mypie2, width=0.5, edgecolor='black')\nplt.margins(0,0)\n\nplt.tight_layout()\nplt.show()\n\n\n\n#ax = sns.countplot(x=\"Category\", hue=\"Type\", data=new_df)\n\n#df.groupby(['Category', 'Type']).count()['App'].unstack().plot(kind='bar', stacked=True, ax=ax)\n#ylabel = plt.ylabel('Number of apps')\n\n\n#graph\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n\nclf = SVC(kernel = 'linear').fit(x_train,y_train)\nclf.predict(x_train)\ny_pred = clf.predict(x_test)\n\n\ncm = confusion_matrix(y_test, y_pred) \n\ncm_df = pd.DataFrame(gps,\n                     columns = ['Positive','Negative','Neutral'], \n                     index = ['Positive','Negative','Neutral'])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True)\n\nplt.ylabel('y_pred')\nplt.xlabel('y_true')\nplt.show()\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"8b9plPRqfsmW","colab_type":"code","outputId":"cbfadd2e-c5cf-4360-f696-b548c784ee17","colab":{},"trusted":true},"cell_type":"code","source":"Sentiment=final['Sentiment'].value_counts()\nprint(Sentiment)\nplt.figure(figsize=(12,12))\nsns.barplot(Sentiment.index, Sentiment.values, alpha=0.8)\nplt.title('Content Rating vs No Apps')\nplt.ylabel('Apps')\nplt.xlabel('Content Rating')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"XBoD9Vtpfsma","colab_type":"code","outputId":"f13b50bd-b078-432d-e503-88849cc50e38","colab":{},"trusted":true},"cell_type":"code","source":"\nimport re\ni=0;\nfor sent in final['Translated_Review'].values:\n    if (len(re.findall('<.*?>', sent))):\n        print(i)\n        print(sent)\n        break;\n    i += 1;","execution_count":null,"outputs":[]},{"metadata":{"id":"tpE7h2AXfsme","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english')) \nsno = nltk.stem.SnowballStemmer('english') \n\ndef cleanhtml(sentence): \n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', sentence)\n    return cleantext\ndef cleanpunc(sentence): \n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n    return  cleaned","execution_count":null,"outputs":[]},{"metadata":{"id":"3DcKkJKFfsmh","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"\ni=0\nstr1=' '\nfinal_string=[]\nall_positive_words=[] \nall_negative_words=[] \nall_netural_words=[]\ns=''\nfor sent in final['Translated_Review'].values:\n    filtered_sentence=[]\n    #print(sent);\n    sent=cleanhtml(sent) \n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n                if(cleaned_words.lower() not in stop):\n                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n                    filtered_sentence.append(s)\n                    if (final['Sentiment'].values)[i] == 'Positive': \n                        all_positive_words.append(s) \n                    if(final['Sentiment'].values)[i] == 'Negative':\n                        all_negative_words.append(s) \n                    if(final['Sentiment'].values)[i] == 'Negative':\n                        all_netural_words.append(s)    \n                else:\n                    continue\n            else:\n                continue \n   \n    str1 = b\" \".join(filtered_sentence) \n    #print(\"***********************************************************************\")\n    \n    final_string.append(str1)\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"id":"tk8nX46afsmk","colab_type":"code","outputId":"3035b659-8216-4085-88d0-5c6753675c28","colab":{},"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(width = 800, height = 800, \n            background_color ='white').generate(str(all_positive_words))\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"tGjNfoOvfsmp","colab_type":"code","outputId":"641ce5f3-c0f5-4c77-e3e8-4a2b7e13aa15","colab":{},"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(width = 800, height = 800, \n            background_color ='white').generate(str(all_negative_words))\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"FSdBFOETfsmu","colab_type":"code","outputId":"7df80a2f-587c-4f43-eae7-66548df75a3a","colab":{},"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(width = 800, height = 800, \n            background_color ='white').generate(str(all_netural_words))\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"CGR9o5Alfsm8","colab_type":"code","outputId":"8ae29caa-6631-4d64-96a6-41f41ec1463e","colab":{},"trusted":true},"cell_type":"code","source":"final.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"AZ2-HH4sfsnG","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx=final[\"Translated_Review\"]\ny=final[\"Sentiment\"]\nx_tr,x_test,y_tr,y_test=train_test_split(x, y, test_size=0.2,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"MA5ZI6QcfsnI","colab_type":"code","outputId":"98835639-fcda-4423-ee75-f7a089c18130","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"print(x_tr.shape,x_test.shape,y_tr.shape,y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"QshU1p5ofsnM","colab_type":"text"},"cell_type":"markdown","source":"## Bag of Words (BoW)"},{"metadata":{"id":"080tnaF6fsnM","colab_type":"text"},"cell_type":"markdown","source":"##  Unigram"},{"metadata":{"id":"BikslYXsfsnN","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#BOW for unigram\nbow = CountVectorizer()\nx_tr_uni = bow.fit_transform(x_tr)\nx_test_uni= bow.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"Lb8odFEefsnb","colab_type":"text"},"cell_type":"markdown","source":"## Bi-Grams\n"},{"metadata":{"id":"wwFfh_lZfsnb","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"bi_gram = CountVectorizer(ngram_range=(1,2))\nx_tr_bi = bi_gram.fit_transform(x_tr)\nx_test_bi = bi_gram.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"KvDP714Kfsng","colab_type":"text"},"cell_type":"markdown","source":"#  TF-IDF"},{"metadata":{"id":"_FjWQNZZfsnh","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\nx_tr_tfidf = tf_idf_vect.fit_transform(x_tr)\nx_test_tfidf = tf_idf_vect.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"Il2eo4nxZTio","colab_type":"text"},"cell_type":"markdown","source":"## Models\n"},{"metadata":{"id":"lEKYp8oXeIEC","colab_type":"text"},"cell_type":"markdown","source":"## NAIVE BAYES"},{"metadata":{"id":"C4mQDJ63r6gq","colab_type":"text"},"cell_type":"markdown","source":"###  Unigram+ TF-IDF"},{"metadata":{"id":"uY5GpxavfsnP","colab_type":"code","outputId":"577dcbe7-0020-4dc0-a29a-f681addadc9c","colab":{"base_uri":"https://localhost:8080/","height":604},"trusted":true},"cell_type":"code","source":"NB = MultinomialNB()\nalpha=[1000,500,100,50,10,5,1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001]\nalpha_value = {'alpha':alpha} #params we need to try on classifier\ngsv = GridSearchCV(NB,alpha_value,cv=5,verbose=1,scoring='f1_micro')\ngsv.fit(x_tr_uni,y_tr)\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(gsv.best_score_)\noptimal_alpha=gsv.best_params_['alpha']\n\n\nx=[]\nplt.figure(figsize=(8,8))\nfor a in gsv.cv_results_['mean_test_score']:\n    x.append(a)\nplt.xlabel(r\"$\\alpha$\",fontsize=15)\nplt.ylabel(\"F1-Score\")\nplt.title(r'F1-Score v/s $\\alpha$')\nplt.plot(alpha,x,linestyle='dashed', marker='x', markerfacecolor='red', markersize=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"a_PFjg_dfsnV","colab_type":"code","outputId":"4f725dea-da99-4f11-ffd2-14ac78c93a61","colab":{"base_uri":"https://localhost:8080/","height":364},"trusted":true},"cell_type":"code","source":"NB = MultinomialNB(1)\nNB.fit(x_tr_uni,y_tr)\ny_pred = NB.predict(x_test_uni)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred,average='weighted')))\nprint(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred,average='weighted')))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred,average='weighted')))\nlabels = ['-ve','0','+ve']\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(3),range(3))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g',xticklabels=labels, yticklabels=labels)","execution_count":null,"outputs":[]},{"metadata":{"id":"u3-s03SZsGfv","colab_type":"text"},"cell_type":"markdown","source":"###  Bi-Gram+TF-IDF"},{"metadata":{"id":"DkgcC90pB0z4","colab_type":"code","outputId":"fecf4c0e-08b3-47a2-8a92-4ad54e6ae54a","colab":{"base_uri":"https://localhost:8080/","height":615},"trusted":true},"cell_type":"code","source":"NB = MultinomialNB()\nalpha=[1000,500,100,50,10,5,1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001]\nalpha_value = {'alpha':alpha} #params we need to try on classifier\ngsv = GridSearchCV(NB,alpha_value,cv=5,verbose=1,scoring='f1_micro')\ngsv.fit(x_tr_bi,y_tr)\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(gsv.best_score_)\noptimal_alpha=gsv.best_params_['alpha']\n\n\nx=[]\nplt.figure(figsize=(8,8))\nfor a in gsv.cv_results_['mean_test_score']:\n    x.append(a)\nplt.xlabel(r\"$\\alpha$\",fontsize=15)\nplt.ylabel(\"F1-Score\")\nplt.title(r'F1-Score v/s $\\alpha$')\nplt.plot(alpha,x,linestyle='dashed', marker='x', markerfacecolor='red', markersize=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"gMEEWeWOB2Gp","colab_type":"code","outputId":"4d8a274e-a314-4157-887c-c265aec17804","colab":{"base_uri":"https://localhost:8080/","height":364},"trusted":true},"cell_type":"code","source":"NB = MultinomialNB(1)\nNB.fit(x_tr_bi,y_tr)\ny_pred = NB.predict(x_test_bi)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred,average='weighted')))\nprint(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred,average='weighted')))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred,average='weighted')))\nlabels = ['-ve','0','+ve']\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(3),range(3))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g',xticklabels=labels, yticklabels=labels)","execution_count":null,"outputs":[]},{"metadata":{"id":"vQO65L65sO_X","colab_type":"text"},"cell_type":"markdown","source":"### TF-IDF"},{"metadata":{"id":"fhgTyQBtCLFt","colab_type":"code","outputId":"5b39cf13-0898-43b3-f7c8-a5b76520fa7d","colab":{"base_uri":"https://localhost:8080/","height":615},"trusted":true},"cell_type":"code","source":"NB = MultinomialNB()\nalpha=[1000,500,100,50,10,5,1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001]\nalpha_value = {'alpha':alpha} #params we need to try on classifier\ngsv = GridSearchCV(NB,alpha_value,cv=5,verbose=1,scoring='f1_micro')\ngsv.fit(x_tr_tfidf,y_tr)\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(gsv.best_score_)\noptimal_alpha=gsv.best_params_['alpha']\n\n\nx=[]\nplt.figure(figsize=(8,8))\nfor a in gsv.cv_results_['mean_test_score']:\n    x.append(a)\nplt.xlabel(r\"$\\alpha$\",fontsize=15)\nplt.ylabel(\"F1-Score\")\nplt.title(r'F1-Score v/s $\\alpha$')\nplt.plot(alpha,x,linestyle='dashed', marker='x', markerfacecolor='red', markersize=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"LhH3_2hbYxuC","colab_type":"code","outputId":"43119315-d661-4a5a-a359-156dbe9a6cc9","colab":{"base_uri":"https://localhost:8080/","height":364},"trusted":true},"cell_type":"code","source":"NB = MultinomialNB(0.1)\nNB.fit(x_tr_tfidf,y_tr)\ny_pred = NB.predict(x_test_tfidf)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred,average='weighted')))\nprint(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred,average='weighted')))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred,average='weighted')))\nlabels = ['-ve','0','+ve']\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(3),range(3))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g',xticklabels=labels, yticklabels=labels)","execution_count":null,"outputs":[]},{"metadata":{"id":"X0x6PUehc5qR","colab_type":"text"},"cell_type":"markdown","source":"##  LR"},{"metadata":{"id":"PKOJsGau4whw","colab_type":"text"},"cell_type":"markdown","source":"###  Uni-gram + TF-IDF"},{"metadata":{"id":"JiYGVIJjtDHr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":614},"outputId":"196b83de-6ff5-4803-90b6-adc0d0d2e972","trusted":true},"cell_type":"code","source":"LR = LogisticRegression(penalty='l1')\nC=[10**-4, 10**-2, 10**0, 10**2, 10**4]\nC_value = [{'C': C}]\n\ngsv = GridSearchCV(LR,C_value,cv=5,verbose=1,scoring='f1_micro')\ngsv.fit(x_tr_uni,y_tr)\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(gsv.best_score_)\noptimal_C=gsv.best_params_['C']\n\n\nx=[]\nplt.figure(figsize=(8,8))\nfor a in gsv.cv_results_['mean_test_score']:\n    x.append(a)\nplt.xlabel(r\"C\",fontsize=15)\nplt.ylabel(\"F1-Score\")\nplt.title(r'F1-Score v/s C')\nplt.plot(C,x,linestyle='dashed', marker='x', markerfacecolor='red', markersize=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"zErjv-xQzM03","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":364},"outputId":"c149dffd-8e0e-42a5-893d-187ebeb5f3e1","trusted":true},"cell_type":"code","source":"LR= LogisticRegression(penalty='l1',C=1)\nLR.fit(x_tr_uni,y_tr)\ny_pred =LR.predict(x_test_uni)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred,average='macro')))\nprint(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred,average='macro')))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred,average='macro')))\nlabels = ['-ve','0','+ve']\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(3),range(3))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g',xticklabels=labels, yticklabels=labels)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"3Ny2uQfv05hD"},"cell_type":"markdown","source":"> ###  Bi-Gram+TF-IDF"},{"metadata":{"id":"eXg3Kprg06vd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":614},"outputId":"3af053f3-2c8a-44c2-a7c8-1c9eaaf8072a","trusted":true},"cell_type":"code","source":"LR = LogisticRegression(penalty='l1')\nC=[10**-4, 10**-2, 10**0, 10**2, 10**4]\nC_value = [{'C': C}]\n\ngsv = GridSearchCV(LR,C_value,cv=5,verbose=1,scoring='f1_micro')\ngsv.fit(x_tr_bi,y_tr)\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(gsv.best_score_)\noptimal_C=gsv.best_params_['C']\n\n\nx=[]\nplt.figure(figsize=(8,8))\nfor a in gsv.cv_results_['mean_test_score']:\n    x.append(a)\nplt.xlabel(\"F1-Score\",fontsize=15)\nplt.ylabel(\"F1-Score\")\nplt.title('F1-Score v/s C')\nplt.plot(C,x,linestyle='dashed', marker='x', markerfacecolor='red', markersize=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"cXnrnHbH07GI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":364},"outputId":"d7bc57bd-9a4d-4e38-da7c-0b3340df6d58","trusted":true},"cell_type":"code","source":"LR= LogisticRegression(penalty='l1',C=100)\nLR.fit(x_tr_bi,y_tr)\ny_pred =LR.predict(x_test_bi)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred,average='macro')))\nprint(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred,average='macro')))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred,average='macro')))\nlabels = ['-ve','0','+ve']\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(3),range(3))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g',xticklabels=labels, yticklabels=labels)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"WCMLsYaX05hF"},"cell_type":"markdown","source":"###  Trigram+TF-IDF"},{"metadata":{"id":"QtN037_N073R","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":614},"outputId":"ac48a462-8573-4940-8458-0604fbcd6506","trusted":true},"cell_type":"code","source":"LR = LogisticRegression(penalty='l1')\nC=[10**-4, 10**-2, 10**0, 10**2, 10**4]\nC_value = [{'C': C}]\n\ngsv = GridSearchCV(LR,C_value,cv=5,verbose=1,scoring='f1_micro')\ngsv.fit(x_tr_tfidf,y_tr)\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(gsv.best_score_)\noptimal_C=gsv.best_params_['C']\n\n\nx=[]\nplt.figure(figsize=(8,8))\nfor a in gsv.cv_results_['mean_test_score']:\n    x.append(a)\nplt.xlabel(\"F1-Score\",fontsize=15)\nplt.ylabel(\"F1-Score\")\nplt.title('F1-Score v/s alpha')\nplt.plot(C,x,linestyle='dashed', marker='x', markerfacecolor='red', markersize=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"shARUhPs08MM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":364},"outputId":"e9be3a18-9a3f-4c73-9e12-4e710c0782aa","trusted":true},"cell_type":"code","source":"LR= LogisticRegression(penalty='l1',C=100)\nLR.fit(x_tr_tfidf,y_tr)\ny_pred =LR.predict(x_test_tfidf)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred,average='macro')))\nprint(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred,average='macro')))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred,average='macro')))\nlabels = ['-ve','0','+ve']\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(3),range(3))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g',xticklabels=labels, yticklabels=labels)","execution_count":null,"outputs":[]},{"metadata":{"id":"hQeVsOzq35ch","colab_type":"text"},"cell_type":"markdown","source":"### SVM"},{"metadata":{"colab_type":"text","id":"cac5Wpze4Afx"},"cell_type":"markdown","source":"### Unigram+TF-IDF"},{"metadata":{"id":"SioWFQCJ4Ksf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":556},"outputId":"ad2e1ea5-d5b4-4502-d97b-601178cbf7c7","trusted":true},"cell_type":"code","source":"LR = SGDClassifier(loss = 'hinge', class_weight='balanced', learning_rate='optimal',eta0=0.001, n_jobs = -1)\na_val = [0.0000001,0.0000005,0.000001,0.000005,0.00001,0.00005,0.0001,0.0005,0.001,0.005,0.01,0.05,0.1]\na_value = [{'alpha': a_val}]\n\ngsv = GridSearchCV(LR,a_value,cv=5,verbose=1,scoring='f1_micro')\ngsv.fit(x_tr_uni,y_tr)\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(gsv.best_score_)\noptimal_alpha=gsv.best_params_['alpha']\n\n\nx=[]\nplt.figure(figsize=(8,8))\nfor a in gsv.cv_results_['mean_test_score']:\n    x.append(a)\nplt.xlabel(\"F1-Score\",fontsize=15)\nplt.ylabel(\"F1-Score\")\nplt.title('F1-Score v/s alpha')\nplt.plot(a_val,x,linestyle='dashed', marker='x', markerfacecolor='red', markersize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"27CgKM164LL_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":364},"outputId":"463be1cf-3d80-4b95-9167-fbd85c4ac23d","trusted":true},"cell_type":"code","source":"clf = SGDClassifier(loss = 'hinge', alpha = 0.0001, class_weight='balanced', learning_rate='optimal',eta0=0.001, n_jobs = -1) \nclf.fit(x_tr_uni,y_tr)\ny_pred = clf.predict(x_test_uni)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred,average='macro')))\nprint(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred,average='macro')))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred,average='macro')))\nlabels = ['-ve','0','+ve']\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(3),range(3))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16},fmt='g',xticklabels=labels, yticklabels=labels)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"u31hebDd4Af1"},"cell_type":"markdown","source":"### Bi-Gram+ TF-IDF"},{"metadata":{"id":"BkEeC4L24LvK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":556},"outputId":"acc48196-3691-4c16-8fdd-8a27e7213aa9","trusted":true},"cell_type":"code","source":"LR = SGDClassifier(loss = 'hinge', class_weight='balanced', learning_rate='optimal',eta0=0.001, n_jobs = -1)\na_val = [0.0000001,0.0000005,0.000001,0.000005,0.00001,0.00005,0.0001,0.0005,0.001,0.005,0.01,0.05,0.1]\na_value = [{'alpha': a_val}]\n\ngsv = GridSearchCV(LR,a_value,cv=5,verbose=1,scoring='f1_micro')\ngsv.fit(x_tr_bi,y_tr)\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(gsv.best_score_)\noptimal_alpha=gsv.best_params_['alpha']\n\n\nx=[]\nplt.figure(figsize=(8,8))\nfor a in gsv.cv_results_['mean_test_score']:\n    x.append(a)\nplt.xlabel(\"F1-Score\",fontsize=15)\nplt.ylabel(\"F1-Score\")\nplt.title('F1-Score v/s alpha')\nplt.plot(a_val,x,linestyle='dashed', marker='x', markerfacecolor='red', markersize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"RfC639d84MHH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":364},"outputId":"d1766927-9d9f-47cd-bd78-1fd36362127e","trusted":true},"cell_type":"code","source":"clf = SGDClassifier(loss = 'hinge', alpha = 0.0001, class_weight='balanced', learning_rate='optimal',eta0=0.001, n_jobs = -1) \nclf.fit(x_tr_bi,y_tr)\ny_pred = clf.predict(x_test_bi)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred,average='macro')))\nprint(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred,average='macro')))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred,average='macro')))\nlabels = ['-ve','0','+ve']\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(3),range(3))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16},fmt='g',xticklabels=labels, yticklabels=labels)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"XV3EJFfG4Af2"},"cell_type":"markdown","source":"###  Trigram+TF-IDF"},{"metadata":{"id":"fmwZ8lgB4Jsv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":556},"outputId":"7eb87bcf-1ac3-4d13-a38c-85719a6bc6db","trusted":true},"cell_type":"code","source":"LR = SGDClassifier(loss = 'hinge', class_weight='balanced', learning_rate='optimal',eta0=0.001, n_jobs = -1)\na_val = [0.0000001,0.0000005,0.000001,0.000005,0.00001,0.00005,0.0001,0.0005,0.001,0.005,0.01,0.05,0.1]\na_value = [{'alpha': a_val}]\n\ngsv = GridSearchCV(LR,a_value,cv=5,verbose=1,scoring='f1_micro')\ngsv.fit(x_tr_tfidf,y_tr)\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(gsv.best_score_)\noptimal_alpha=gsv.best_params_['alpha']\n\n\nx=[]\nplt.figure(figsize=(8,8))\nfor a in gsv.cv_results_['mean_test_score']:\n    x.append(a)\nplt.xlabel(\"F1-Score\",fontsize=15)\nplt.ylabel(\"F1-Score\")\nplt.title('F1-Score v/s alpha')\nplt.plot(a_val,x,linestyle='dashed', marker='x', markerfacecolor='red', markersize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"djbMbm1U4MlF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":364},"outputId":"826a6216-cbc7-4d56-ef2a-f711fdebc703","trusted":true,"collapsed":true},"cell_type":"code","source":"clf = SGDClassifier(loss = 'hinge', alpha = 0.00005, class_weight='balanced', learning_rate='optimal',eta0=0.001, n_jobs = -1) \nclf.fit(x_tr_tfidf,y_tr)\ny_pred = clf.predict(x_test_tfidf)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred,average='macro')))\nprint(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred,average='macro')))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred,average='macro')))\nlabels = ['-ve','0','+ve']\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(3),range(3))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16},fmt='g',xticklabels=labels, yticklabels=labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## BAGGING\n\n\nscaler = StandardScaler()\nX_train_stan = scaler.fit_transform(X_train)\nX_test_stan = scaler.transform(X_test)\nbagReg = BaggingRegressor(LinearRegression(), n_estimators = 500, \n                           bootstrap_features = True, max_features = 1.0, oob_score = True, random_state = 0)\n\nbagReg.fit(X_train_stan, y_train)\nprint(\"score is: \", bagReg.oob_score_) \n\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n                        ngram_range=(1, 2), \n                        stop_words='english')\n\n\nfeatures = tfidf.fit_transform(df2.Consumer_complaint).toarray()\n\nlabels = df2.category_id\n\nprint(\"Each of the %d ratings is represented by %d intalls (TF-IDF score of unigrams and bigrams and trigram)\" %(features.shape))\n\n\nfor ratings, category_id in sorted(category.items()):\n  features_chi2 = chi2(features, labels == category)\n  indices = np.argsort(features_chi2[0])\n  feature_names = np.array(tfidf.get_feature_names())[indices]\n  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n  print(\"\\n==> %s:\" %(ratings))\n  print(\"  * Most Correlated Unigrams are: %s\" %(', '.join(unigrams[-N:])))\n  print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(bigrams[-N:])))\n  print(\"  * Most Correlated Trirams are: %s\" %(', '.join(bigrams[-N:])))\n\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## KNN\n\nfrom sklearn.neighbors import KNeighborsClassifier\nn_splits = 10  \n\nneigh_max = 100\ntscv = TimeSeriesSplit(n_splits=n_splits)\n\nk_acc = []\n\nfor k in range(1,neigh_max,2):\n    \n    acc_list = []\n    for train, cv in tscv.split(X_train):\n          if(train.size > k): \n            knn = KNeighborsClassifier(n_neighbors=k,algorithm='brute',n_jobs=-1)\n            knn.fit(X_train[train],y_train[train])\n            acc_list.append(knn.score(X_train[cv],y_train[cv])*100)\n    if(acc_list):        \n        acc_nparr = np.array(acc_list) \n    k_acc.append(acc_nparr.mean())\nk_acc = np.array(k_acc)\nk_acc_uni_gram = openfromfile(\"k_acc_uni_gram\")\nk_acc_uni_gram\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=11)*\nknn.fit(X_train,y_train)\ny_pred = knn.predict(X_test)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred)))\nprint(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred)))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n\n\n\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## tf-idf+bigram and tf-idf+trigram\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\nX_train, X_test, y_train, y_test = train_test_split(df_sample['CleanedText'].values,df_sample['Score'].values,test_size=0.3,shuffle=False)\n\n\nbi_gram = CountVectorizer(ngram_range=(1,2)) \nX_train = bi_gram.fit_transform(X_train)\n\nX_train = preprocessing.normalize(X_train)\nprint(\"Train Data Size: \",X_train.shape)\nX_test = bi_gram.transform(X_test)\n\nX_test = preprocessing.normalize(X_test)\nprint(\"Test Data Size: \",X_test.shape)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred)))\nprint(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred)))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\n\nX_train, X_test, y_train, y_test = train_test_split(df_sample['CleanedText'].values,df_sample['Score'].values,test_size=0.3,shuffle=False)\n\ntfidf = TfidfVectorizer(ngram_range=(1,2)) \nX_train = tfidf.fit_transform(X_train)\n\nX_train = preprocessing.normalize(X_train)\nprint(\"Train Data Size: \",X_train.shape)\nX_test = tfidf.transform(X_test)\n\nX_test = preprocessing.normalize(X_test)\nprint(\"Test Data Size: \",X_test.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"GyRyawEmAWJV","colab_type":"text"},"cell_type":"markdown","source":"\n\n<b>1.Naive Bayes</b>\n\n|Model |hyper parameter|F1-score test|Accuracy Test|\n|------|------|------|------|------|\n|uni-gram+TF-IDF|1|0.72|80.00|\n|bi-gram+TF-IDF |1|0.62|82.14|\n|trigram+TF-IDF|0.01| 0.63|82.21|\n\n\n<b>2.Logistic Regression</b>\n\n|Model|hyper parameter|F1-score test|Accuracy Test|\n|------|------|------|------|------|------|\n|uni-gram+TF-IDF|1|0.86|84.08|\n|bi-gram+TF-IDF|100| 0.85|84.61|\n| trigram+TF-IDF|100| 0.85|84.48|\n\n<b>3. SVM</b>\n\n|Model |hyper parameter|F1-score test|Accuracy Test|\n|------|------|------|------|------|------|\n|uni-gram + TF-IDF|0.0001|0.89|92.89|\n|bi-gram+TF-IDF|0.0001|0.89|93.41|\n|tri-gram+TF-IDF |0.00005|0.88|93.37|\n\n\n<b>4. KNN</b>\n\n|Model |hyper parameter|F1-score test|Accuracy Test|\n|------|------|------|------|------|------|\n|uni-gram + TF-IDF|0.0001|0.69|91.01|\n|bi-gram+TF-IDF|0.0001|0.68|90.90|\n|tri-gram+TF-IDF |0.0005|0.70|88.39|\n\n\n<b>5. RF</b>\n\n|Model |hyper parameter|F1-score test|Accuracy Test|\n|------|------|------|------|------|------|\n|uni-gram + TF-IDF|0.0001|0.68|83.42|\n|bi-gram+TF-IDF|0.0001|0.61|85.11|\n|tri-gram+TF-IDF |0.00005|0.62|84.16|\n\n<b>6. Bagging LR</b>\n\n|Model |hyper parameter|F1-score test|Accuracy Test|\n|------|------|------|------|------|------|\n|uni-gram + TF-IDF|0.0001|0.87|86.50|\n|bi-gram+TF-IDF|0.0001|0.86|86.50|\n|tri-gram+TF-IDF |0.00005|0.86|87.88|\n\n\n\n<b>7. Bagging NB</b>\n\n|Model |hyper parameter|F1-score test|Accuracy Test|\n|------|------|------|------|------|------|\n|uni-gram + TF-IDF|0.0001|0.75|85.50|\n|bi-gram+TF-IDF|0.0001|0.76|85.11|\n|tri-gram+TF-IDF |0.00005|0.76|84.00|\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}],"metadata":{"colab":{"name":"Sentiment Analysis of GooglePlayStore DataSet.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}